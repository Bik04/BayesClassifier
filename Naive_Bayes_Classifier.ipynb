{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vn_uToAmgvGF"
   },
   "source": [
    "**Class Exercise**: Show of hands for 99%? 50-99%? 10-50%? less than 10%?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epbNL37NgvGI"
   },
   "source": [
    "Bayes rule is notoriously counter-intuitive when posed with problems like this.\n",
    "\n",
    "Let's work through the formula:\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$\n",
    "\n",
    "In this case, $P(A|B)$ denotes the probability of being a user (A), given a positive test (B).\n",
    "\n",
    "So we can re-write the formula in the language of our problem:\n",
    "\n",
    "$$ P(user|positive) = \\frac{P(positive|user)P(user)}{P(positive)} $$\n",
    "\n",
    "$P(positive|user)$ is the probability of a positive test, given a that the subject is a user, $P(user)$ is the probability of being a user.\n",
    "\n",
    "$P(positive)$ is the probability of a positive test - In our case, it is the probability of a user scoring a positive, multiplied by the probability of being a user, plus the probability of a positive test for a non-user, multiplied by the probability of not being a user.\n",
    "\n",
    "Let's do the math:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BEYgS6fvgvGI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33221476510067094\n"
     ]
    }
   ],
   "source": [
    "p_user = 0.005\n",
    "p_nonuser = 1 - p_user\n",
    "p_user_pos = 0.99\n",
    "p_nonuser_negative = 0.99\n",
    "p_nonuser_pos = 1 - p_nonuser_negative\n",
    "\n",
    "prob = (p_user_pos * p_user)/(p_user_pos * p_user + p_nonuser_pos * p_nonuser)\n",
    "\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7MZa7Zi8gvGL"
   },
   "source": [
    "If we run this test again on the same police officer, we would update our prior belief that they have used the drug to be 0.33. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tmhAhr0_gvGL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9799040191961608\n"
     ]
    }
   ],
   "source": [
    "# Ok, so if we run this test again on the same police officer, we\n",
    "# would update our prior belief that they have used the drug to be 0.33\n",
    "\n",
    "p_user = 0.33\n",
    "p_nonuser = 1 - p_user\n",
    "p_user_pos = 0.99\n",
    "p_nonuser_negative = 0.99\n",
    "p_nonuser_pos = 1 - p_nonuser_negative\n",
    "\n",
    "prob = (p_user_pos * p_user)/(p_user_pos * p_user + p_nonuser_pos * p_nonuser)\n",
    "\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FH2qtJ6HgvGN"
   },
   "source": [
    "This is a problem! If we had simply done the test and told people that they were 99% likely to have taken the drug, we would be completely wrong.\n",
    "\n",
    "What we have done is used a prior (in this case the p(user)) to make sure that we are sensible about interpreting our test. We know that drug use in rare in our population, so the majority of positive tests come from false positives.\n",
    "\n",
    "Sensitivity and Specificity are the terms commonly used in medical diagnosis - in the above example our sensitivity is the probability that we detect a true drug user (0.99), and the specificity is the probability of correctly identifying a non-user (also 0.99 in our example, but not necessarily even). The  rarity of a disease will often cause false positives - you can imagine even if we have a test for a disease which is 99% for both specificity and sensitivity if the disease is rare, the majority of positives will be errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w8dmaeJ1gvGO"
   },
   "source": [
    "---\n",
    "\n",
    "#### Exercise 1\n",
    "\n",
    "\n",
    "The world population in 2020 was ~7,800,000,000 people on the planet. Out of those, 5,732,780 people were diagnosed with disease X. We have a new test potentially which identifies people with disease X correctly 99% of the time, and identifies people who do not have it correctly 99.8% of the time.\n",
    "\n",
    "1. What is the probability that someone identified as having disease X truly having disease X?\n",
    "2. Is this a useful model? How many false positives do we expect?\n",
    "3. What recommendations do you have for the modeler - should they focus on false positives, false negatives, or give up?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26690442841625095"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_sick = 5_732_780 / 7_800_000_000\n",
    "p_not_sick = 1 - p_sick\n",
    "\n",
    "p_positive_if_sick = 0.99\n",
    "p_negative_if_sick = 1 - p_positive_if_sick\n",
    "\n",
    "p_negative_if_not_sick = 0.998\n",
    "p_positive_if_not_sick = 1 - p_negative_if_not_sick\n",
    "\n",
    "# Remind ourselves of Bayes' rule\n",
    "# p_sick_if_positive = (p_positive_if_sick * p_sick) / p_positive\n",
    "\n",
    "# Need to calculate p_positive\n",
    "p_positive = p_not_sick*p_positive_if_not_sick + p_sick*p_positive_if_sick\n",
    "\n",
    "\n",
    "# Apply Bayes' rule\n",
    "p_sick_if_positive = (p_positive_if_sick * p_sick) / p_positive\n",
    "p_sick_if_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15577077"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 - How many false positives do we expect?\n",
    "\n",
    "# Let's say we tested every person in the world who isn't sick. \n",
    "# How many false positives do we expect?\n",
    "\n",
    "round(p_positive_if_not_sick * p_not_sick * (7_800_000_000 - 5_732_780))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bvxLnIuJgvGR"
   },
   "source": [
    "### Naive Bayes Classification\n",
    "\n",
    "Naive Bayes is a set of algorithms for classifying data based on *features*. In the simplest case, given a set of data, we can decide which class it belongs to based on its attributes. You can imagine the model looking at mammals vs birds, and finding that the $P(bird|wings)$ is high, with some $(mammal|wings)$ from bats. It will carry out the probability for each independent variable we give, and then find the most likely class.\n",
    "\n",
    "The Naive Bayes classifier determines what *class* that instances from a new set of data belongs to, given previous data it has observed and learned from. Suppose we have many classes $C_1,C_2,\\ldots,C_n$, and we represent the set of data to be classified as $\\textbf{x} =  [x_1, x_2, \\cdots , x_k]$.  The probability that the given data $\\textbf{x}$ belongs to class $C_i$ is given by\n",
    "\n",
    "$$ P(C_i\\,|\\,\\textbf{x}) = \\frac{P(C_i)P(\\textbf{x}\\,|\\,C_i)}{P(\\textbf{x})}$$\n",
    "\n",
    "We will carry out a few simple examples - Naive Bayes Classifiers are often called a machine learning method, which we will be talking about in a couple weeks time.\n",
    "\n",
    "You can read the documentation here:\n",
    "http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "Let's use an example dataset, to classify fruits versus vegetables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "67yly83OgvGR"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Used in salads</th>\n",
       "      <th>Grows Underground</th>\n",
       "      <th>Served cooked</th>\n",
       "      <th>Needs Peeling</th>\n",
       "      <th>Fruit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Used in salads  Grows Underground  Served cooked  Needs Peeling  Fruit\n",
       "0               0                  0              0              0      1\n",
       "1               1                  0              1              0      1\n",
       "2               0                  1              1              1      0\n",
       "3               1                  1              0              1      0\n",
       "4               0                  0              0              1      1\n",
       "5               0                  1              1              1      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# data:\n",
    "columns = ['Used in salads', 'Grows Underground', 'Served cooked', 'Needs Peeling', 'Fruit']\n",
    "\n",
    "apple = [0,0,0,0,1]\n",
    "tomato = [1,0,1,0,1]\n",
    "potato = [0,1,1,1,0]\n",
    "carrot = [1,1,0,1,0]\n",
    "banana = [0,0,0,1,1]\n",
    "turnip = [0,1,1,1,0]\n",
    "\n",
    "data = pd.DataFrame([apple, tomato, potato, carrot, banana, turnip], columns = columns)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lKbcgQWkgvGU"
   },
   "source": [
    "From here, we will use the classifier to figure out the probabilities. Under the hood, here is effectively what is happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XgnvyoX9gvGU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([1, 3, 2, 3]), 1: array([1, 0, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "x = data.iloc[:,:-1].values\n",
    "y = data.iloc[:,-1].values\n",
    "\n",
    "counts = {}\n",
    "for label in np.unique(y):\n",
    "    counts[label] = x[y == label].sum(axis = 0)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aVhr7_BVgvGX"
   },
   "source": [
    "The classifier then compares the frequency of each feature of $X$ to each label in $y$, and calculates a probability that a new item with that attribute belongs to the class.\n",
    "\n",
    "We use BernoulliNB here, as we have all 1/0 data - GaussianNB stores mean and standard deviation (sd) of each value for a continuous variables, and Mulitnomial allows multiple categorical values. When we see new data, we use the rule to classify it into the new bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FJBmzOmAgvGX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "# Instatiate our model\n",
    "nbmodel = BernoulliNB()\n",
    "# Fit our model\n",
    "nbmodel.fit(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ueCzYPrBgvGa"
   },
   "source": [
    "Lets say we have a new edible object where we want to predict whether it's a fruit or a vegetable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ueCzYPrBgvGa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.42857143 0.57142857]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# new data to be classified will have values for each column, but not\n",
    "# our target column. (in_salads, underground, cooked, peeling)\n",
    "new_data = [1, 0, 1, 1] # e.g. a pineapple\n",
    "\n",
    "# reformat our data as a 1xn array\n",
    "new_data = np.array(new_data).reshape(1, -1)\n",
    "\n",
    "print(nbmodel.predict_proba(new_data) )#get probabilistic prediction\n",
    "print(nbmodel.predict(new_data) )#get hard prediction\n",
    "\n",
    "# nbmodel.feature_count_ \n",
    "#print(\"score\",nbmodel.score(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 3., 2., 3.],\n",
       "       [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbmodel.feature_count_"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bayesian Statistics.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
